{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to load a darknet dataset into a DeepView-Dataset Reader for Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, \"D:\\\\work\\\\au-zone\\\\tasks\\\\validator\\\\deepview-datasets\\\\\")\n",
    "os.environ['PYTHONPATH'] = \"D:\\\\work\\\\au-zone\\\\tasks\\\\validator\\\\deepview-datasets\\\\\"\n",
    "\n",
    "from deepview.datasets.readers import TFDarknetDetectionReader\n",
    "from deepview.datasets.iterators import TFObjectDetectionIterator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the TF dataset reader that wraps a Darknet dataset reader.\n",
    "\n",
    "The `images` parameter expects a location with all the *jpg, *jpeg, or *png files. For the case of the `annotations`,\n",
    "the parameter expects the path to a folder containing txt files with the same names than images. If any annotation does not exist, \n",
    "the image will be loaded without them.\n",
    "\n",
    "Notice that Labels are passed as a list of strings but It could be either of:\n",
    "- A txt file containing the names of the labels per row\n",
    "- A yaml file like described by ModelPack dataset formats (with a filed named `classes`)\n",
    "- A yaml file like the one used for YoloV5: https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml\n",
    "- A yaml file like the one used for YoloV7: https://github.com/WongKinYiu/yolov7/blob/main/data/coco.yaml\n",
    "\n",
    "This reader will load the images and annotations in TensorFlow, allowing the training iterators to connect and batch samples on GPU\n",
    "\n",
    "\"\"\"\n",
    "reader = TFDarknetDetectionReader(\n",
    "    images=\"playingcards/dataset/images/train\",\n",
    "    annotations=\"playingcards/dataset/labels/train\",\n",
    "    classes=[\"ace\", \"nine\", \"six\", \"four\", \"eight\", \"queen\", \"seven\", \"king\", \"ten\", \"jack\", \"five\", \"two\", \"three\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Iterator orchestrates the batching and augmentation techniques along the reader.\n",
    "\"\"\"\n",
    "handler = TFObjectDetectionIterator(\n",
    "    from_config=\"playingcards/dataset.yaml\",\n",
    "    shape=(320, 320, 3)\n",
    ")\n",
    "iterator = handler.get_train_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can iterate along the iterator. Notice that batch is not defined, so elements won't be batched.\n",
    "\"\"\"\n",
    "for image, boxes in iterator:\n",
    "    print(image.shape, boxes.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
